---
title: "A guide to using the record cleaner, donwloading files, and extracting crops the of classification images"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{A guide to extracting the crops the of classification images}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(knitr)
```

In this Vignette, we will:
1. Use the record cleaner to filter species identifications that are extreme, given their known phenology and range of occurence, as well as species that are hard or impossible to identify by automated monitoring methods that capture images.

### Setup

In the first stage, we will load all of our functions
```{r}
# Install EntoInsights
# devtools::install_github("AMI-system/EntoInsights")

# Alternatively, If you are running this within the EntoInsights source code, you can run:
# devtools::load_all()

# First we will load our dependancies
library(EntoInsights)
library(lubridate)
library(dplyr)
library(stringr)
library(ggplot2)
```

### Using the record cleaner
The data we will use in this tutorial is the first 100 rows of a automated monitoring dataset in the UK. This dataset is loaded into your library alongside the EntoInsights package.
```{r}
# Load an example classification dataset
str(classification_df)
```

The record cleaner function is called `append_RC_rules`. The function id designed to ingest datasets output from the species classification workflow, like the `classification_df` object. To use the `append_RC_rules` function, you need the deployment's latitude and longitude.

The record cleaner uses data from the National Biodiversity Network (NBN) Record Cleaner. Please reference them in your results. Please see the following link for more information: https://nbn.org.uk/tools-and-resources/nbn-record-cleaner/

```{r}
# Deployment position
longitude = -1.6448439
latitude = 51.6171655

classification_df_rc_rules = append_RC_rules(classification_df, longitude, latitude)
```

Compare the differences between the original classification dataframe and what we have now:

```{r}
# What have we added
str(classification_df)
str(classification_df_rc_rules)
```

As you can see, append_RC_rules, has inserted the following rows:
1. `top_x_species_identification_difficulty`: A number, from 1 - 4, indicating the degree to which the species is difficult to identify. Please see the key below:

```{r, echo=FALSE}
df <- data.frame(
  Grade = 1:5,
  Description = c(
    "Generally distinctive species unlikely to be confused with other species. Records usually accepted within the normal flight period or geographical range; otherwise one or more good quality photographs, or possibly a specimen, may be required. Applies to species which will quickly become familiar to those gaining experience.",
    "Can be confused with other species. Should not present any difficulty for experienced recorders; however, good quality photographs or a specimen will be required if the recorder is not familiar with the species, and experienced recorders may be expected to provide additional evidence if the species record is outside of the normal flight period or geographical range. Worn specimens (esp. micro-moths) are likely to be unidentifiable unless a specimen is retained for microscopic examination.",
    "Very scarce species or species that are difficult to identify. A voucher specimen may be required by your County Recorder (seek advice before releasing the moth) particularly if the species record is outside of the normal flight period or geographical range. A good set of photographs showing all critical identification features clearly may suffice in some cases.",
    "Species that can only be identified following critical assessment often involving dissection and examination of genitalia. A specimen should be retained for confirmation.",
    "Ungraded."
  )
)
kable(df, col.names = c("Grade", "Description"))
```

Note that due to the nature of the record cleaner function, species with 5 or NA values are ungraded.

2. `top_x_species_presence`: Whether the species is present, with respect to its known activity. Note that species with NA values not included as entries in the Record cleaner rules.
3. `top_x_species_within_date`: Whether the species is present, with respect to its known phenology. Note that species with NA values not included as entries in the Record cleaner rules.

The number after top_ matches the number of top predictions in the classification workflow. In our example dataset, there is 5 top predictions.

You can use the results of the record cleaner to subset depending on your research needs. For example, we could subset out species that are at least somewhat visually distinctive. Note this also removes ungraded species (with values of 5 or NA).
```{r}
classification_df_rc_rules = classification_df_rc_rules %>%
filter(top_1_species_identification_difficulty < 3)
```

### Donwloading snapshots

*IMPORTANT:* The stages below require a credentials file. Please obtain one first from me and dylcar@ceh.ac.uk
2. Download data, both snapshot images, and audible files, from the object store.
3. From the snapshot images, we will extract crops of of choice species.

Say you are handling some unusual classifications and you want to see their snapshots or crops. Or alternatively, you would like to obtain a subsample of crops for a small subset of days of sampling. The download and cropping functionality in this package, can allow you to download these files. Please note that this is an accessible but not particularly fast solution to downloading files. If you would like to donwload more than 1000 files, please see https://github.com/AMI-system/object-store-scripts/tree/image_download

This process requires an object store credentials json file. To access this file in json format. If you would like to generate this json, please run the following with your access key and secret key:
```{r}
# Create your credentials
# create_credentials("my_access_key", "my_secret_key")
```

This will save a credentials file within your working directory. Now you can download your classifications!

Lets first isolate a subset of snapshot images
```{r}
classifications_subset = classification_df_rc_rules %>% slice_sample(n = 10)
```

To download snapshots, please use the `download_object_store_files` function. As input, you will need to provide:
1. The bucket name (bucket): So far, this is always the ISO-3 code of the country, in lower case, e.g., 'gbr'.
2. deployment_id: The ID associated with the bucket. If you are uncertain of this, see the image_path of your classifications. The ID should be in the path. For example, 'dep000072' in './data/solar/gbr/dep000072/20240618024000-snapshot_crop0.jpg'
3. The data type (data_type): This can be either a single value stating the data type. This is either a single value or a vector of values for each file (if you want to download multiple files).
4. The filenames of desired files (filename): This is the basename of the files. This will always be called 'image_path' inside the classifications tables. To obtain the basename, please use the basename() function.
5. The classification_id: This will almost always be equal to the row number, however it is important to generate manually as it ensures that your crops can be cropped and correctly linked with your classifications.

There are other parameters you can provide. If you wish to know more of these, please see the function documentation.
```{r}
# Create a classification ID
classifications_subset = classifications_subset %>% mutate(classification_id = row_number())

# Parameters for download
bucket = "gbr"
deployment_id = "dep000072"
data_type = "snapshot_images"
filenames = basename(classifications_subset$image_path)
classification_id = classifications_subset$classification_id

snapshot_downloads <- download_object_store_files(
  bucket = bucket,
  deployment_id = deployment_id,
  data_type = data_type,
  filename = filenames,
  classification_id
)
```

As raised in the warning, please use your credentials carefully, as they give you write access to the contents of the object store.

You will have received the following outputs from this process:
1. Snapshots in a 'downloads' directory: The snapshots are sorted into subfolders named after the deployment ID.
2. files_downloaded_log.csv: This logs your deployment IDs, file types, and the local installation path. There is also a classification_id column. This is simply a number that will be used later to pair snapshots to crops, if you choose to generate them. They also ensure that files do not overwrite each other when the crops are generated. This is the function output, and is also saved as a files_downloaded_log.csv.

Likewise, if we want to download audio, we can run similar commands:
```{r}
# Parameters for download
bucket = "gbr"
deployment_id = "dep000072"
data_type = "ultrasound_recordings" # this can alternatively be audible_recordings

# Some file examples
ultrasound_files = c("dep000072/ultrasound_recordings/SOLAR_20240716_002935.wav",
"dep000072/ultrasound_recordings/SOLAR_20240716_013254.wav",
"dep000072/ultrasound_recordings/SOLAR_20240716_013748.wav",
"dep000072/ultrasound_recordings/SOLAR_20240716_021307.wav")

# extract the basenames
filenames = basename(ultrasound_files)

download_object_store_files(
  bucket = "gbr",
  deployment_id = deployment_id,
  data_type = data_type,
  filename = filenames,
  classification_id = 1:length(ultrasound_files)
)
```

To obtain crops, we need to provide the download log output from the `download_object_store_files` in the previous section, as well our classifications dataframe. The pairing is used to ensure that donwloaded snapshots are correctly linked to classifications in the dataframe, and generated crops.

```{r}
# crop_and_save_images(snapshot_downloads,
#   classifications_subset,
#   output_dir = "./crops") 
```